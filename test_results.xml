<?xml version="1.0" encoding="utf-8"?><testsuites><testsuite name="pytest" errors="0" failures="10" skipped="4" tests="18" time="21.195" timestamp="2025-05-09T23:13:05.628717-10:00" hostname="SamanthasAir95.lan"><testcase classname="tests.test_ddp" name="test_DistributedDataParallelCPU[0.0016-ToyModel]" time="1.686"><failure message="torch.multiprocessing.spawn.ProcessRaisedException: &#10;&#10;-- Process 1 terminated with the following error:&#10;Traceback (most recent call last):&#10;  File &quot;/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py&quot;, line 68, in _wrap&#10;    fn(i, *args)&#10;  File &quot;/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/cs336-systems/tests/test_ddp.py&quot;, line 75, in _test_DistributedDataParallelCPU&#10;    ddp_model = get_ddp_bucketed(&#10;                ^^^^^^^^^^^^^^^^^&#10;  File &quot;/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/cs336-systems/tests/adapters.py&quot;, line 478, in get_ddp_bucketed&#10;    raise NotImplementedError&#10;NotImplementedError">bucket_size_mb = 0.0016, model_class = &lt;class 'tests.common.ToyModel'&gt;

    @pytest.mark.parametrize("model_class", [ToyModel, ToyModelWithTiedWeights])
    @pytest.mark.parametrize("bucket_size_mb", [0.0016, 0.0001, 0.01])
    def test_DistributedDataParallelCPU(bucket_size_mb, model_class):
        """
        bucket_size_mb 0.0016 is designed to test the case with 2 buckets (one bucket
        has 2 parameter tensors, the other has 2).
    
        bucket_size_mb 0.0001 is designed to test the case with 3 buckets (each bucket
        has one parameter tensors).
    
        bucket_size_mb 0.01 is designed to test the case with 1 buckets (containing
        3 parameter tensors).
        """
        world_size = 2
&gt;       mp.spawn(
            _test_DistributedDataParallelCPU,
            args=(world_size, bucket_size_mb, model_class),
            nprocs=world_size,
            join=True,
        )

cs336-systems/tests/test_ddp.py:44: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:241: in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
    while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = &lt;torch.multiprocessing.spawn.ProcessContext object at 0x10ca4b2f0&gt;, timeout = None

    def join(self, timeout=None):
        r"""Join one or more processes within spawn context.
    
        Attempt to join one or more processes in this spawn context.
        If one of them exited with a non-zero exit status, this function
        kills the remaining processes and raises an exception with the cause
        of the first process exiting.
    
        Returns ``True`` if all processes have been joined successfully,
        ``False`` if there are more processes that need to be joined.
    
        Args:
            timeout (float): Wait this long before giving up on waiting.
        """
        # Ensure this function can be called even when we're done.
        if len(self.sentinels) == 0:
            return True
    
        # Wait for any process to fail or all of them to succeed.
        ready = multiprocessing.connection.wait(
            self.sentinels.keys(),
            timeout=timeout,
        )
    
        error_index = None
        for sentinel in ready:
            index = self.sentinels.pop(sentinel)
            process = self.processes[index]
            process.join()
            if process.exitcode != 0:
                error_index = index
                break
    
        # Return if there was no error.
        if error_index is None:
            # Return whether or not all processes have been joined.
            return len(self.sentinels) == 0
    
        # Assume failure. Terminate processes that are still alive.
        for process in self.processes:
            if process.is_alive():
                process.terminate()
            process.join()
    
        # There won't be an error on the queue if the process crashed.
        failed_process = self.processes[error_index]
        if self.error_queues[error_index].empty():
            exitcode = self.processes[error_index].exitcode
            if exitcode &lt; 0:
                name = signal.Signals(-exitcode).name
                raise ProcessExitedException(
                    "process %d terminated with signal %s" % (error_index, name),
                    error_index=error_index,
                    error_pid=failed_process.pid,
                    exit_code=exitcode,
                    signal_name=name,
                )
            else:
                raise ProcessExitedException(
                    "process %d terminated with exit code %d" % (error_index, exitcode),
                    error_index=error_index,
                    error_pid=failed_process.pid,
                    exit_code=exitcode,
                )
    
        original_trace = self.error_queues[error_index].get()
        msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
        msg += original_trace
&gt;       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException: 
E       
E       -- Process 1 terminated with the following error:
E       Traceback (most recent call last):
E         File "/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/cs336-systems/tests/test_ddp.py", line 75, in _test_DistributedDataParallelCPU
E           ddp_model = get_ddp_bucketed(
E                       ^^^^^^^^^^^^^^^^^
E         File "/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/cs336-systems/tests/adapters.py", line 478, in get_ddp_bucketed
E           raise NotImplementedError
E       NotImplementedError

336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException</failure></testcase><testcase classname="tests.test_ddp" name="test_DistributedDataParallelCPU[0.0016-ToyModelWithTiedWeights]" time="1.134"><failure message="torch.multiprocessing.spawn.ProcessRaisedException: &#10;&#10;-- Process 1 terminated with the following error:&#10;Traceback (most recent call last):&#10;  File &quot;/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py&quot;, line 68, in _wrap&#10;    fn(i, *args)&#10;  File &quot;/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/cs336-systems/tests/test_ddp.py&quot;, line 75, in _test_DistributedDataParallelCPU&#10;    ddp_model = get_ddp_bucketed(&#10;                ^^^^^^^^^^^^^^^^^&#10;  File &quot;/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/cs336-systems/tests/adapters.py&quot;, line 478, in get_ddp_bucketed&#10;    raise NotImplementedError&#10;NotImplementedError">bucket_size_mb = 0.0016, model_class = &lt;class 'tests.common.ToyModelWithTiedWeights'&gt;

    @pytest.mark.parametrize("model_class", [ToyModel, ToyModelWithTiedWeights])
    @pytest.mark.parametrize("bucket_size_mb", [0.0016, 0.0001, 0.01])
    def test_DistributedDataParallelCPU(bucket_size_mb, model_class):
        """
        bucket_size_mb 0.0016 is designed to test the case with 2 buckets (one bucket
        has 2 parameter tensors, the other has 2).
    
        bucket_size_mb 0.0001 is designed to test the case with 3 buckets (each bucket
        has one parameter tensors).
    
        bucket_size_mb 0.01 is designed to test the case with 1 buckets (containing
        3 parameter tensors).
        """
        world_size = 2
&gt;       mp.spawn(
            _test_DistributedDataParallelCPU,
            args=(world_size, bucket_size_mb, model_class),
            nprocs=world_size,
            join=True,
        )

cs336-systems/tests/test_ddp.py:44: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:241: in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
    while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = &lt;torch.multiprocessing.spawn.ProcessContext object at 0x10e670da0&gt;, timeout = None

    def join(self, timeout=None):
        r"""Join one or more processes within spawn context.
    
        Attempt to join one or more processes in this spawn context.
        If one of them exited with a non-zero exit status, this function
        kills the remaining processes and raises an exception with the cause
        of the first process exiting.
    
        Returns ``True`` if all processes have been joined successfully,
        ``False`` if there are more processes that need to be joined.
    
        Args:
            timeout (float): Wait this long before giving up on waiting.
        """
        # Ensure this function can be called even when we're done.
        if len(self.sentinels) == 0:
            return True
    
        # Wait for any process to fail or all of them to succeed.
        ready = multiprocessing.connection.wait(
            self.sentinels.keys(),
            timeout=timeout,
        )
    
        error_index = None
        for sentinel in ready:
            index = self.sentinels.pop(sentinel)
            process = self.processes[index]
            process.join()
            if process.exitcode != 0:
                error_index = index
                break
    
        # Return if there was no error.
        if error_index is None:
            # Return whether or not all processes have been joined.
            return len(self.sentinels) == 0
    
        # Assume failure. Terminate processes that are still alive.
        for process in self.processes:
            if process.is_alive():
                process.terminate()
            process.join()
    
        # There won't be an error on the queue if the process crashed.
        failed_process = self.processes[error_index]
        if self.error_queues[error_index].empty():
            exitcode = self.processes[error_index].exitcode
            if exitcode &lt; 0:
                name = signal.Signals(-exitcode).name
                raise ProcessExitedException(
                    "process %d terminated with signal %s" % (error_index, name),
                    error_index=error_index,
                    error_pid=failed_process.pid,
                    exit_code=exitcode,
                    signal_name=name,
                )
            else:
                raise ProcessExitedException(
                    "process %d terminated with exit code %d" % (error_index, exitcode),
                    error_index=error_index,
                    error_pid=failed_process.pid,
                    exit_code=exitcode,
                )
    
        original_trace = self.error_queues[error_index].get()
        msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
        msg += original_trace
&gt;       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException: 
E       
E       -- Process 1 terminated with the following error:
E       Traceback (most recent call last):
E         File "/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/cs336-systems/tests/test_ddp.py", line 75, in _test_DistributedDataParallelCPU
E           ddp_model = get_ddp_bucketed(
E                       ^^^^^^^^^^^^^^^^^
E         File "/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/cs336-systems/tests/adapters.py", line 478, in get_ddp_bucketed
E           raise NotImplementedError
E       NotImplementedError

336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException</failure></testcase><testcase classname="tests.test_ddp" name="test_DistributedDataParallelCPU[0.0001-ToyModel]" time="1.133"><failure message="torch.multiprocessing.spawn.ProcessRaisedException: &#10;&#10;-- Process 0 terminated with the following error:&#10;Traceback (most recent call last):&#10;  File &quot;/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py&quot;, line 68, in _wrap&#10;    fn(i, *args)&#10;  File &quot;/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/cs336-systems/tests/test_ddp.py&quot;, line 75, in _test_DistributedDataParallelCPU&#10;    ddp_model = get_ddp_bucketed(&#10;                ^^^^^^^^^^^^^^^^^&#10;  File &quot;/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/cs336-systems/tests/adapters.py&quot;, line 478, in get_ddp_bucketed&#10;    raise NotImplementedError&#10;NotImplementedError">bucket_size_mb = 0.0001, model_class = &lt;class 'tests.common.ToyModel'&gt;

    @pytest.mark.parametrize("model_class", [ToyModel, ToyModelWithTiedWeights])
    @pytest.mark.parametrize("bucket_size_mb", [0.0016, 0.0001, 0.01])
    def test_DistributedDataParallelCPU(bucket_size_mb, model_class):
        """
        bucket_size_mb 0.0016 is designed to test the case with 2 buckets (one bucket
        has 2 parameter tensors, the other has 2).
    
        bucket_size_mb 0.0001 is designed to test the case with 3 buckets (each bucket
        has one parameter tensors).
    
        bucket_size_mb 0.01 is designed to test the case with 1 buckets (containing
        3 parameter tensors).
        """
        world_size = 2
&gt;       mp.spawn(
            _test_DistributedDataParallelCPU,
            args=(world_size, bucket_size_mb, model_class),
            nprocs=world_size,
            join=True,
        )

cs336-systems/tests/test_ddp.py:44: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:241: in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
    while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = &lt;torch.multiprocessing.spawn.ProcessContext object at 0x10e671e80&gt;, timeout = None

    def join(self, timeout=None):
        r"""Join one or more processes within spawn context.
    
        Attempt to join one or more processes in this spawn context.
        If one of them exited with a non-zero exit status, this function
        kills the remaining processes and raises an exception with the cause
        of the first process exiting.
    
        Returns ``True`` if all processes have been joined successfully,
        ``False`` if there are more processes that need to be joined.
    
        Args:
            timeout (float): Wait this long before giving up on waiting.
        """
        # Ensure this function can be called even when we're done.
        if len(self.sentinels) == 0:
            return True
    
        # Wait for any process to fail or all of them to succeed.
        ready = multiprocessing.connection.wait(
            self.sentinels.keys(),
            timeout=timeout,
        )
    
        error_index = None
        for sentinel in ready:
            index = self.sentinels.pop(sentinel)
            process = self.processes[index]
            process.join()
            if process.exitcode != 0:
                error_index = index
                break
    
        # Return if there was no error.
        if error_index is None:
            # Return whether or not all processes have been joined.
            return len(self.sentinels) == 0
    
        # Assume failure. Terminate processes that are still alive.
        for process in self.processes:
            if process.is_alive():
                process.terminate()
            process.join()
    
        # There won't be an error on the queue if the process crashed.
        failed_process = self.processes[error_index]
        if self.error_queues[error_index].empty():
            exitcode = self.processes[error_index].exitcode
            if exitcode &lt; 0:
                name = signal.Signals(-exitcode).name
                raise ProcessExitedException(
                    "process %d terminated with signal %s" % (error_index, name),
                    error_index=error_index,
                    error_pid=failed_process.pid,
                    exit_code=exitcode,
                    signal_name=name,
                )
            else:
                raise ProcessExitedException(
                    "process %d terminated with exit code %d" % (error_index, exitcode),
                    error_index=error_index,
                    error_pid=failed_process.pid,
                    exit_code=exitcode,
                )
    
        original_trace = self.error_queues[error_index].get()
        msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
        msg += original_trace
&gt;       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException: 
E       
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/cs336-systems/tests/test_ddp.py", line 75, in _test_DistributedDataParallelCPU
E           ddp_model = get_ddp_bucketed(
E                       ^^^^^^^^^^^^^^^^^
E         File "/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/cs336-systems/tests/adapters.py", line 478, in get_ddp_bucketed
E           raise NotImplementedError
E       NotImplementedError

336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException</failure></testcase><testcase classname="tests.test_ddp" name="test_DistributedDataParallelCPU[0.0001-ToyModelWithTiedWeights]" time="1.144"><failure message="torch.multiprocessing.spawn.ProcessRaisedException: &#10;&#10;-- Process 1 terminated with the following error:&#10;Traceback (most recent call last):&#10;  File &quot;/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py&quot;, line 68, in _wrap&#10;    fn(i, *args)&#10;  File &quot;/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/cs336-systems/tests/test_ddp.py&quot;, line 75, in _test_DistributedDataParallelCPU&#10;    ddp_model = get_ddp_bucketed(&#10;                ^^^^^^^^^^^^^^^^^&#10;  File &quot;/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/cs336-systems/tests/adapters.py&quot;, line 478, in get_ddp_bucketed&#10;    raise NotImplementedError&#10;NotImplementedError">bucket_size_mb = 0.0001, model_class = &lt;class 'tests.common.ToyModelWithTiedWeights'&gt;

    @pytest.mark.parametrize("model_class", [ToyModel, ToyModelWithTiedWeights])
    @pytest.mark.parametrize("bucket_size_mb", [0.0016, 0.0001, 0.01])
    def test_DistributedDataParallelCPU(bucket_size_mb, model_class):
        """
        bucket_size_mb 0.0016 is designed to test the case with 2 buckets (one bucket
        has 2 parameter tensors, the other has 2).
    
        bucket_size_mb 0.0001 is designed to test the case with 3 buckets (each bucket
        has one parameter tensors).
    
        bucket_size_mb 0.01 is designed to test the case with 1 buckets (containing
        3 parameter tensors).
        """
        world_size = 2
&gt;       mp.spawn(
            _test_DistributedDataParallelCPU,
            args=(world_size, bucket_size_mb, model_class),
            nprocs=world_size,
            join=True,
        )

cs336-systems/tests/test_ddp.py:44: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:241: in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
    while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = &lt;torch.multiprocessing.spawn.ProcessContext object at 0x10e672f30&gt;, timeout = None

    def join(self, timeout=None):
        r"""Join one or more processes within spawn context.
    
        Attempt to join one or more processes in this spawn context.
        If one of them exited with a non-zero exit status, this function
        kills the remaining processes and raises an exception with the cause
        of the first process exiting.
    
        Returns ``True`` if all processes have been joined successfully,
        ``False`` if there are more processes that need to be joined.
    
        Args:
            timeout (float): Wait this long before giving up on waiting.
        """
        # Ensure this function can be called even when we're done.
        if len(self.sentinels) == 0:
            return True
    
        # Wait for any process to fail or all of them to succeed.
        ready = multiprocessing.connection.wait(
            self.sentinels.keys(),
            timeout=timeout,
        )
    
        error_index = None
        for sentinel in ready:
            index = self.sentinels.pop(sentinel)
            process = self.processes[index]
            process.join()
            if process.exitcode != 0:
                error_index = index
                break
    
        # Return if there was no error.
        if error_index is None:
            # Return whether or not all processes have been joined.
            return len(self.sentinels) == 0
    
        # Assume failure. Terminate processes that are still alive.
        for process in self.processes:
            if process.is_alive():
                process.terminate()
            process.join()
    
        # There won't be an error on the queue if the process crashed.
        failed_process = self.processes[error_index]
        if self.error_queues[error_index].empty():
            exitcode = self.processes[error_index].exitcode
            if exitcode &lt; 0:
                name = signal.Signals(-exitcode).name
                raise ProcessExitedException(
                    "process %d terminated with signal %s" % (error_index, name),
                    error_index=error_index,
                    error_pid=failed_process.pid,
                    exit_code=exitcode,
                    signal_name=name,
                )
            else:
                raise ProcessExitedException(
                    "process %d terminated with exit code %d" % (error_index, exitcode),
                    error_index=error_index,
                    error_pid=failed_process.pid,
                    exit_code=exitcode,
                )
    
        original_trace = self.error_queues[error_index].get()
        msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
        msg += original_trace
&gt;       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException: 
E       
E       -- Process 1 terminated with the following error:
E       Traceback (most recent call last):
E         File "/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/cs336-systems/tests/test_ddp.py", line 75, in _test_DistributedDataParallelCPU
E           ddp_model = get_ddp_bucketed(
E                       ^^^^^^^^^^^^^^^^^
E         File "/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/cs336-systems/tests/adapters.py", line 478, in get_ddp_bucketed
E           raise NotImplementedError
E       NotImplementedError

336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException</failure></testcase><testcase classname="tests.test_ddp" name="test_DistributedDataParallelCPU[0.01-ToyModel]" time="1.110"><failure message="torch.multiprocessing.spawn.ProcessRaisedException: &#10;&#10;-- Process 0 terminated with the following error:&#10;Traceback (most recent call last):&#10;  File &quot;/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py&quot;, line 68, in _wrap&#10;    fn(i, *args)&#10;  File &quot;/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/cs336-systems/tests/test_ddp.py&quot;, line 75, in _test_DistributedDataParallelCPU&#10;    ddp_model = get_ddp_bucketed(&#10;                ^^^^^^^^^^^^^^^^^&#10;  File &quot;/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/cs336-systems/tests/adapters.py&quot;, line 478, in get_ddp_bucketed&#10;    raise NotImplementedError&#10;NotImplementedError">bucket_size_mb = 0.01, model_class = &lt;class 'tests.common.ToyModel'&gt;

    @pytest.mark.parametrize("model_class", [ToyModel, ToyModelWithTiedWeights])
    @pytest.mark.parametrize("bucket_size_mb", [0.0016, 0.0001, 0.01])
    def test_DistributedDataParallelCPU(bucket_size_mb, model_class):
        """
        bucket_size_mb 0.0016 is designed to test the case with 2 buckets (one bucket
        has 2 parameter tensors, the other has 2).
    
        bucket_size_mb 0.0001 is designed to test the case with 3 buckets (each bucket
        has one parameter tensors).
    
        bucket_size_mb 0.01 is designed to test the case with 1 buckets (containing
        3 parameter tensors).
        """
        world_size = 2
&gt;       mp.spawn(
            _test_DistributedDataParallelCPU,
            args=(world_size, bucket_size_mb, model_class),
            nprocs=world_size,
            join=True,
        )

cs336-systems/tests/test_ddp.py:44: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:241: in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
    while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = &lt;torch.multiprocessing.spawn.ProcessContext object at 0x10e6d8890&gt;, timeout = None

    def join(self, timeout=None):
        r"""Join one or more processes within spawn context.
    
        Attempt to join one or more processes in this spawn context.
        If one of them exited with a non-zero exit status, this function
        kills the remaining processes and raises an exception with the cause
        of the first process exiting.
    
        Returns ``True`` if all processes have been joined successfully,
        ``False`` if there are more processes that need to be joined.
    
        Args:
            timeout (float): Wait this long before giving up on waiting.
        """
        # Ensure this function can be called even when we're done.
        if len(self.sentinels) == 0:
            return True
    
        # Wait for any process to fail or all of them to succeed.
        ready = multiprocessing.connection.wait(
            self.sentinels.keys(),
            timeout=timeout,
        )
    
        error_index = None
        for sentinel in ready:
            index = self.sentinels.pop(sentinel)
            process = self.processes[index]
            process.join()
            if process.exitcode != 0:
                error_index = index
                break
    
        # Return if there was no error.
        if error_index is None:
            # Return whether or not all processes have been joined.
            return len(self.sentinels) == 0
    
        # Assume failure. Terminate processes that are still alive.
        for process in self.processes:
            if process.is_alive():
                process.terminate()
            process.join()
    
        # There won't be an error on the queue if the process crashed.
        failed_process = self.processes[error_index]
        if self.error_queues[error_index].empty():
            exitcode = self.processes[error_index].exitcode
            if exitcode &lt; 0:
                name = signal.Signals(-exitcode).name
                raise ProcessExitedException(
                    "process %d terminated with signal %s" % (error_index, name),
                    error_index=error_index,
                    error_pid=failed_process.pid,
                    exit_code=exitcode,
                    signal_name=name,
                )
            else:
                raise ProcessExitedException(
                    "process %d terminated with exit code %d" % (error_index, exitcode),
                    error_index=error_index,
                    error_pid=failed_process.pid,
                    exit_code=exitcode,
                )
    
        original_trace = self.error_queues[error_index].get()
        msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
        msg += original_trace
&gt;       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException: 
E       
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/cs336-systems/tests/test_ddp.py", line 75, in _test_DistributedDataParallelCPU
E           ddp_model = get_ddp_bucketed(
E                       ^^^^^^^^^^^^^^^^^
E         File "/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/cs336-systems/tests/adapters.py", line 478, in get_ddp_bucketed
E           raise NotImplementedError
E       NotImplementedError

336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException</failure></testcase><testcase classname="tests.test_ddp" name="test_DistributedDataParallelCPU[0.01-ToyModelWithTiedWeights]" time="1.113"><failure message="torch.multiprocessing.spawn.ProcessRaisedException: &#10;&#10;-- Process 0 terminated with the following error:&#10;Traceback (most recent call last):&#10;  File &quot;/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py&quot;, line 68, in _wrap&#10;    fn(i, *args)&#10;  File &quot;/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/cs336-systems/tests/test_ddp.py&quot;, line 75, in _test_DistributedDataParallelCPU&#10;    ddp_model = get_ddp_bucketed(&#10;                ^^^^^^^^^^^^^^^^^&#10;  File &quot;/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/cs336-systems/tests/adapters.py&quot;, line 478, in get_ddp_bucketed&#10;    raise NotImplementedError&#10;NotImplementedError">bucket_size_mb = 0.01, model_class = &lt;class 'tests.common.ToyModelWithTiedWeights'&gt;

    @pytest.mark.parametrize("model_class", [ToyModel, ToyModelWithTiedWeights])
    @pytest.mark.parametrize("bucket_size_mb", [0.0016, 0.0001, 0.01])
    def test_DistributedDataParallelCPU(bucket_size_mb, model_class):
        """
        bucket_size_mb 0.0016 is designed to test the case with 2 buckets (one bucket
        has 2 parameter tensors, the other has 2).
    
        bucket_size_mb 0.0001 is designed to test the case with 3 buckets (each bucket
        has one parameter tensors).
    
        bucket_size_mb 0.01 is designed to test the case with 1 buckets (containing
        3 parameter tensors).
        """
        world_size = 2
&gt;       mp.spawn(
            _test_DistributedDataParallelCPU,
            args=(world_size, bucket_size_mb, model_class),
            nprocs=world_size,
            join=True,
        )

cs336-systems/tests/test_ddp.py:44: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:241: in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
    while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = &lt;torch.multiprocessing.spawn.ProcessContext object at 0x10e6d9940&gt;, timeout = None

    def join(self, timeout=None):
        r"""Join one or more processes within spawn context.
    
        Attempt to join one or more processes in this spawn context.
        If one of them exited with a non-zero exit status, this function
        kills the remaining processes and raises an exception with the cause
        of the first process exiting.
    
        Returns ``True`` if all processes have been joined successfully,
        ``False`` if there are more processes that need to be joined.
    
        Args:
            timeout (float): Wait this long before giving up on waiting.
        """
        # Ensure this function can be called even when we're done.
        if len(self.sentinels) == 0:
            return True
    
        # Wait for any process to fail or all of them to succeed.
        ready = multiprocessing.connection.wait(
            self.sentinels.keys(),
            timeout=timeout,
        )
    
        error_index = None
        for sentinel in ready:
            index = self.sentinels.pop(sentinel)
            process = self.processes[index]
            process.join()
            if process.exitcode != 0:
                error_index = index
                break
    
        # Return if there was no error.
        if error_index is None:
            # Return whether or not all processes have been joined.
            return len(self.sentinels) == 0
    
        # Assume failure. Terminate processes that are still alive.
        for process in self.processes:
            if process.is_alive():
                process.terminate()
            process.join()
    
        # There won't be an error on the queue if the process crashed.
        failed_process = self.processes[error_index]
        if self.error_queues[error_index].empty():
            exitcode = self.processes[error_index].exitcode
            if exitcode &lt; 0:
                name = signal.Signals(-exitcode).name
                raise ProcessExitedException(
                    "process %d terminated with signal %s" % (error_index, name),
                    error_index=error_index,
                    error_pid=failed_process.pid,
                    exit_code=exitcode,
                    signal_name=name,
                )
            else:
                raise ProcessExitedException(
                    "process %d terminated with exit code %d" % (error_index, exitcode),
                    error_index=error_index,
                    error_pid=failed_process.pid,
                    exit_code=exitcode,
                )
    
        original_trace = self.error_queues[error_index].get()
        msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
        msg += original_trace
&gt;       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException: 
E       
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/cs336-systems/tests/test_ddp.py", line 75, in _test_DistributedDataParallelCPU
E           ddp_model = get_ddp_bucketed(
E                       ^^^^^^^^^^^^^^^^^
E         File "/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/cs336-systems/tests/adapters.py", line 478, in get_ddp_bucketed
E           raise NotImplementedError
E       NotImplementedError

336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException</failure></testcase><testcase classname="tests.test_ddp_individual_parameters" name="test_DistributedDataParallelIndividualParameters[ToyModel]" time="1.137"><failure message="torch.multiprocessing.spawn.ProcessRaisedException: &#10;&#10;-- Process 1 terminated with the following error:&#10;Traceback (most recent call last):&#10;  File &quot;/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py&quot;, line 68, in _wrap&#10;    fn(i, *args)&#10;  File &quot;/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/cs336-systems/tests/test_ddp_individual_parameters.py&quot;, line 60, in _test_DistributedDataParallelIndividualParameters&#10;    ddp_model = get_ddp_individual_parameters(ddp_base)&#10;                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/cs336-systems/tests/adapters.py&quot;, line 440, in get_ddp_individual_parameters&#10;    raise NotImplementedError&#10;NotImplementedError">model_class = &lt;class 'tests.common.ToyModel'&gt;

    @pytest.mark.parametrize("model_class", [ToyModel, ToyModelWithTiedWeights])
    def test_DistributedDataParallelIndividualParameters(model_class):
        world_size = 2
&gt;       mp.spawn(
            _test_DistributedDataParallelIndividualParameters,
            args=(world_size, model_class),
            nprocs=world_size,
            join=True,
        )

cs336-systems/tests/test_ddp_individual_parameters.py:32: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:241: in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
    while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = &lt;torch.multiprocessing.spawn.ProcessContext object at 0x10e672d80&gt;, timeout = None

    def join(self, timeout=None):
        r"""Join one or more processes within spawn context.
    
        Attempt to join one or more processes in this spawn context.
        If one of them exited with a non-zero exit status, this function
        kills the remaining processes and raises an exception with the cause
        of the first process exiting.
    
        Returns ``True`` if all processes have been joined successfully,
        ``False`` if there are more processes that need to be joined.
    
        Args:
            timeout (float): Wait this long before giving up on waiting.
        """
        # Ensure this function can be called even when we're done.
        if len(self.sentinels) == 0:
            return True
    
        # Wait for any process to fail or all of them to succeed.
        ready = multiprocessing.connection.wait(
            self.sentinels.keys(),
            timeout=timeout,
        )
    
        error_index = None
        for sentinel in ready:
            index = self.sentinels.pop(sentinel)
            process = self.processes[index]
            process.join()
            if process.exitcode != 0:
                error_index = index
                break
    
        # Return if there was no error.
        if error_index is None:
            # Return whether or not all processes have been joined.
            return len(self.sentinels) == 0
    
        # Assume failure. Terminate processes that are still alive.
        for process in self.processes:
            if process.is_alive():
                process.terminate()
            process.join()
    
        # There won't be an error on the queue if the process crashed.
        failed_process = self.processes[error_index]
        if self.error_queues[error_index].empty():
            exitcode = self.processes[error_index].exitcode
            if exitcode &lt; 0:
                name = signal.Signals(-exitcode).name
                raise ProcessExitedException(
                    "process %d terminated with signal %s" % (error_index, name),
                    error_index=error_index,
                    error_pid=failed_process.pid,
                    exit_code=exitcode,
                    signal_name=name,
                )
            else:
                raise ProcessExitedException(
                    "process %d terminated with exit code %d" % (error_index, exitcode),
                    error_index=error_index,
                    error_pid=failed_process.pid,
                    exit_code=exitcode,
                )
    
        original_trace = self.error_queues[error_index].get()
        msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
        msg += original_trace
&gt;       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException: 
E       
E       -- Process 1 terminated with the following error:
E       Traceback (most recent call last):
E         File "/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/cs336-systems/tests/test_ddp_individual_parameters.py", line 60, in _test_DistributedDataParallelIndividualParameters
E           ddp_model = get_ddp_individual_parameters(ddp_base)
E                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E         File "/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/cs336-systems/tests/adapters.py", line 440, in get_ddp_individual_parameters
E           raise NotImplementedError
E       NotImplementedError

336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException</failure></testcase><testcase classname="tests.test_ddp_individual_parameters" name="test_DistributedDataParallelIndividualParameters[ToyModelWithTiedWeights]" time="1.140"><failure message="torch.multiprocessing.spawn.ProcessRaisedException: &#10;&#10;-- Process 1 terminated with the following error:&#10;Traceback (most recent call last):&#10;  File &quot;/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py&quot;, line 68, in _wrap&#10;    fn(i, *args)&#10;  File &quot;/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/cs336-systems/tests/test_ddp_individual_parameters.py&quot;, line 60, in _test_DistributedDataParallelIndividualParameters&#10;    ddp_model = get_ddp_individual_parameters(ddp_base)&#10;                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/cs336-systems/tests/adapters.py&quot;, line 440, in get_ddp_individual_parameters&#10;    raise NotImplementedError&#10;NotImplementedError">model_class = &lt;class 'tests.common.ToyModelWithTiedWeights'&gt;

    @pytest.mark.parametrize("model_class", [ToyModel, ToyModelWithTiedWeights])
    def test_DistributedDataParallelIndividualParameters(model_class):
        world_size = 2
&gt;       mp.spawn(
            _test_DistributedDataParallelIndividualParameters,
            args=(world_size, model_class),
            nprocs=world_size,
            join=True,
        )

cs336-systems/tests/test_ddp_individual_parameters.py:32: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:241: in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
    while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = &lt;torch.multiprocessing.spawn.ProcessContext object at 0x10e671280&gt;, timeout = None

    def join(self, timeout=None):
        r"""Join one or more processes within spawn context.
    
        Attempt to join one or more processes in this spawn context.
        If one of them exited with a non-zero exit status, this function
        kills the remaining processes and raises an exception with the cause
        of the first process exiting.
    
        Returns ``True`` if all processes have been joined successfully,
        ``False`` if there are more processes that need to be joined.
    
        Args:
            timeout (float): Wait this long before giving up on waiting.
        """
        # Ensure this function can be called even when we're done.
        if len(self.sentinels) == 0:
            return True
    
        # Wait for any process to fail or all of them to succeed.
        ready = multiprocessing.connection.wait(
            self.sentinels.keys(),
            timeout=timeout,
        )
    
        error_index = None
        for sentinel in ready:
            index = self.sentinels.pop(sentinel)
            process = self.processes[index]
            process.join()
            if process.exitcode != 0:
                error_index = index
                break
    
        # Return if there was no error.
        if error_index is None:
            # Return whether or not all processes have been joined.
            return len(self.sentinels) == 0
    
        # Assume failure. Terminate processes that are still alive.
        for process in self.processes:
            if process.is_alive():
                process.terminate()
            process.join()
    
        # There won't be an error on the queue if the process crashed.
        failed_process = self.processes[error_index]
        if self.error_queues[error_index].empty():
            exitcode = self.processes[error_index].exitcode
            if exitcode &lt; 0:
                name = signal.Signals(-exitcode).name
                raise ProcessExitedException(
                    "process %d terminated with signal %s" % (error_index, name),
                    error_index=error_index,
                    error_pid=failed_process.pid,
                    exit_code=exitcode,
                    signal_name=name,
                )
            else:
                raise ProcessExitedException(
                    "process %d terminated with exit code %d" % (error_index, exitcode),
                    error_index=error_index,
                    error_pid=failed_process.pid,
                    exit_code=exitcode,
                )
    
        original_trace = self.error_queues[error_index].get()
        msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
        msg += original_trace
&gt;       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException: 
E       
E       -- Process 1 terminated with the following error:
E       Traceback (most recent call last):
E         File "/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/cs336-systems/tests/test_ddp_individual_parameters.py", line 60, in _test_DistributedDataParallelIndividualParameters
E           ddp_model = get_ddp_individual_parameters(ddp_base)
E                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E         File "/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/cs336-systems/tests/adapters.py", line 440, in get_ddp_individual_parameters
E           raise NotImplementedError
E       NotImplementedError

336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException</failure></testcase><testcase classname="tests.test_rmsnorm" name="test_rmsnorm_forward_pass_pytorch" time="0.003" /><testcase classname="tests.test_rmsnorm" name="test_rmsnorm_forward_pass_triton" time="0.000"><skipped type="pytest.skip" message="A GPU must be available to run Triton kernels">/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/cs336-systems/tests/test_rmsnorm.py:41: A GPU must be available to run Triton kernels</skipped></testcase><testcase classname="tests.test_rmsnorm" name="test_rmsnorm_backward_x_pytorch" time="0.296" /><testcase classname="tests.test_rmsnorm" name="test_rmsnorm_backward_g_pytorch" time="0.001" /><testcase classname="tests.test_rmsnorm" name="test_rmsnorm_backward_x_triton" time="0.000"><skipped type="pytest.skip" message="A GPU must be available to run Triton kernels">/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/cs336-systems/tests/test_rmsnorm.py:73: A GPU must be available to run Triton kernels</skipped></testcase><testcase classname="tests.test_rmsnorm" name="test_rmsnorm_backward_g_triton" time="0.000"><skipped type="pytest.skip" message="A GPU must be available to run Triton kernels">/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/cs336-systems/tests/test_rmsnorm.py:91: A GPU must be available to run Triton kernels</skipped></testcase><testcase classname="tests.test_rmsnorm" name="test_rmsnorm_autograd_pytorch_forward_backward" time="0.001" /><testcase classname="tests.test_rmsnorm" name="test_rmsnorm_autograd_triton_forward_backward" time="0.000"><skipped type="pytest.skip" message="A GPU must be available to run Triton kernels">/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/cs336-systems/tests/test_rmsnorm.py:137: A GPU must be available to run Triton kernels</skipped></testcase><testcase classname="tests.test_sharded_optimizer" name="test_sharded_optimizer[ToyModel]" time="3.850"><failure message="torch.multiprocessing.spawn.ProcessRaisedException: &#10;&#10;-- Process 1 terminated with the following error:&#10;Traceback (most recent call last):&#10;  File &quot;/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py&quot;, line 68, in _wrap&#10;    fn(i, *args)&#10;  File &quot;/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/cs336-systems/tests/test_sharded_optimizer.py&quot;, line 48, in _test_sharded_optimizer&#10;    sharded_optimizer = get_sharded_optimizer(&#10;                        ^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/cs336-systems/tests/adapters.py&quot;, line 531, in get_sharded_optimizer&#10;    raise NotImplementedError&#10;NotImplementedError">model_class = &lt;class 'tests.common.ToyModel'&gt;

    @pytest.mark.parametrize("model_class", [ToyModel, ToyModelWithTiedWeights])
    def test_sharded_optimizer(model_class):
        world_size = 2
&gt;       mp.spawn(
            _test_sharded_optimizer,
            args=(world_size, model_class),
            nprocs=world_size,
            join=True,
        )

cs336-systems/tests/test_sharded_optimizer.py:22: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:241: in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
    while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = &lt;torch.multiprocessing.spawn.ProcessContext object at 0x11879e3c0&gt;, timeout = None

    def join(self, timeout=None):
        r"""Join one or more processes within spawn context.
    
        Attempt to join one or more processes in this spawn context.
        If one of them exited with a non-zero exit status, this function
        kills the remaining processes and raises an exception with the cause
        of the first process exiting.
    
        Returns ``True`` if all processes have been joined successfully,
        ``False`` if there are more processes that need to be joined.
    
        Args:
            timeout (float): Wait this long before giving up on waiting.
        """
        # Ensure this function can be called even when we're done.
        if len(self.sentinels) == 0:
            return True
    
        # Wait for any process to fail or all of them to succeed.
        ready = multiprocessing.connection.wait(
            self.sentinels.keys(),
            timeout=timeout,
        )
    
        error_index = None
        for sentinel in ready:
            index = self.sentinels.pop(sentinel)
            process = self.processes[index]
            process.join()
            if process.exitcode != 0:
                error_index = index
                break
    
        # Return if there was no error.
        if error_index is None:
            # Return whether or not all processes have been joined.
            return len(self.sentinels) == 0
    
        # Assume failure. Terminate processes that are still alive.
        for process in self.processes:
            if process.is_alive():
                process.terminate()
            process.join()
    
        # There won't be an error on the queue if the process crashed.
        failed_process = self.processes[error_index]
        if self.error_queues[error_index].empty():
            exitcode = self.processes[error_index].exitcode
            if exitcode &lt; 0:
                name = signal.Signals(-exitcode).name
                raise ProcessExitedException(
                    "process %d terminated with signal %s" % (error_index, name),
                    error_index=error_index,
                    error_pid=failed_process.pid,
                    exit_code=exitcode,
                    signal_name=name,
                )
            else:
                raise ProcessExitedException(
                    "process %d terminated with exit code %d" % (error_index, exitcode),
                    error_index=error_index,
                    error_pid=failed_process.pid,
                    exit_code=exitcode,
                )
    
        original_trace = self.error_queues[error_index].get()
        msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
        msg += original_trace
&gt;       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException: 
E       
E       -- Process 1 terminated with the following error:
E       Traceback (most recent call last):
E         File "/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/cs336-systems/tests/test_sharded_optimizer.py", line 48, in _test_sharded_optimizer
E           sharded_optimizer = get_sharded_optimizer(
E                               ^^^^^^^^^^^^^^^^^^^^^^
E         File "/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/cs336-systems/tests/adapters.py", line 531, in get_sharded_optimizer
E           raise NotImplementedError
E       NotImplementedError

336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException</failure></testcase><testcase classname="tests.test_sharded_optimizer" name="test_sharded_optimizer[ToyModelWithTiedWeights]" time="1.695"><failure message="torch.multiprocessing.spawn.ProcessRaisedException: &#10;&#10;-- Process 0 terminated with the following error:&#10;Traceback (most recent call last):&#10;  File &quot;/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py&quot;, line 68, in _wrap&#10;    fn(i, *args)&#10;  File &quot;/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/cs336-systems/tests/test_sharded_optimizer.py&quot;, line 48, in _test_sharded_optimizer&#10;    sharded_optimizer = get_sharded_optimizer(&#10;                        ^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/cs336-systems/tests/adapters.py&quot;, line 531, in get_sharded_optimizer&#10;    raise NotImplementedError&#10;NotImplementedError">model_class = &lt;class 'tests.common.ToyModelWithTiedWeights'&gt;

    @pytest.mark.parametrize("model_class", [ToyModel, ToyModelWithTiedWeights])
    def test_sharded_optimizer(model_class):
        world_size = 2
&gt;       mp.spawn(
            _test_sharded_optimizer,
            args=(world_size, model_class),
            nprocs=world_size,
            join=True,
        )

cs336-systems/tests/test_sharded_optimizer.py:22: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:241: in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
    while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = &lt;torch.multiprocessing.spawn.ProcessContext object at 0x10fe29970&gt;, timeout = None

    def join(self, timeout=None):
        r"""Join one or more processes within spawn context.
    
        Attempt to join one or more processes in this spawn context.
        If one of them exited with a non-zero exit status, this function
        kills the remaining processes and raises an exception with the cause
        of the first process exiting.
    
        Returns ``True`` if all processes have been joined successfully,
        ``False`` if there are more processes that need to be joined.
    
        Args:
            timeout (float): Wait this long before giving up on waiting.
        """
        # Ensure this function can be called even when we're done.
        if len(self.sentinels) == 0:
            return True
    
        # Wait for any process to fail or all of them to succeed.
        ready = multiprocessing.connection.wait(
            self.sentinels.keys(),
            timeout=timeout,
        )
    
        error_index = None
        for sentinel in ready:
            index = self.sentinels.pop(sentinel)
            process = self.processes[index]
            process.join()
            if process.exitcode != 0:
                error_index = index
                break
    
        # Return if there was no error.
        if error_index is None:
            # Return whether or not all processes have been joined.
            return len(self.sentinels) == 0
    
        # Assume failure. Terminate processes that are still alive.
        for process in self.processes:
            if process.is_alive():
                process.terminate()
            process.join()
    
        # There won't be an error on the queue if the process crashed.
        failed_process = self.processes[error_index]
        if self.error_queues[error_index].empty():
            exitcode = self.processes[error_index].exitcode
            if exitcode &lt; 0:
                name = signal.Signals(-exitcode).name
                raise ProcessExitedException(
                    "process %d terminated with signal %s" % (error_index, name),
                    error_index=error_index,
                    error_pid=failed_process.pid,
                    exit_code=exitcode,
                    signal_name=name,
                )
            else:
                raise ProcessExitedException(
                    "process %d terminated with exit code %d" % (error_index, exitcode),
                    error_index=error_index,
                    error_pid=failed_process.pid,
                    exit_code=exitcode,
                )
    
        original_trace = self.error_queues[error_index].get()
        msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
        msg += original_trace
&gt;       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException: 
E       
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/cs336-systems/tests/test_sharded_optimizer.py", line 48, in _test_sharded_optimizer
E           sharded_optimizer = get_sharded_optimizer(
E                               ^^^^^^^^^^^^^^^^^^^^^^
E         File "/Users/samanthakocher/Desktop/ece491b/s2025-assignment4-hpc/cs336-systems/tests/adapters.py", line 531, in get_sharded_optimizer
E           raise NotImplementedError
E       NotImplementedError

336_a2_test_venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException</failure></testcase></testsuite></testsuites>